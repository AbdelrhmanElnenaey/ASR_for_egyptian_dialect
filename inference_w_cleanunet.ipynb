{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"94OEZybMRYr3","outputId":"c024fcf0-c61c-434b-9eae-d7d895c1fbbb"},"outputs":[],"source":["!mkdir ~/.kaggle\n","!cp kaggle.json ~/.kaggle\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5SDg-U4yRdNp"},"outputs":[],"source":["import os\n","import torch\n","from tqdm.auto import tqdm"]},{"cell_type":"markdown","metadata":{"id":"19x5yMiloIuW"},"source":["# Noise Removal"]},{"cell_type":"markdown","metadata":{"id":"AnJODp_pEGgo"},"source":["## Model Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MTJNvHHzhipY"},"outputs":[],"source":["# Copyright (c) 2022 NVIDIA CORPORATION.\n","#   Licensed under the MIT license.\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","def weight_scaling_init(layer):\n","    \"\"\"\n","    weight rescaling initialization from https://arxiv.org/abs/1911.13254\n","    \"\"\"\n","    w = layer.weight.detach()\n","    alpha = 10.0 * w.std()\n","    layer.weight.data /= torch.sqrt(alpha)\n","    layer.bias.data /= torch.sqrt(alpha)\n","\n","\n","# Transformer (encoder) https://github.com/jadore801120/attention-is-all-you-need-pytorch\n","# Original Copyright 2017 Victor Huang\n","#  MIT License (https://opensource.org/licenses/MIT)\n","\n","class ScaledDotProductAttention(nn.Module):\n","    ''' Scaled Dot-Product Attention '''\n","\n","    def __init__(self, temperature, attn_dropout=0.1):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.dropout = nn.Dropout(attn_dropout)\n","\n","    def forward(self, q, k, v, mask=None):\n","\n","        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n","\n","        if mask is not None:\n","            attn = attn.masked_fill(mask == 0, -1e9)\n","\n","        attn = self.dropout(F.softmax(attn, dim=-1))\n","        output = torch.matmul(attn, v)\n","\n","        return output, attn\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    ''' Multi-Head Attention module '''\n","\n","    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n","        super().__init__()\n","\n","        self.n_head = n_head\n","        self.d_k = d_k\n","        self.d_v = d_v\n","\n","        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n","        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n","        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n","        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n","\n","        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n","\n","\n","    def forward(self, q, k, v, mask=None):\n","\n","        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n","        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n","\n","        residual = q\n","\n","        # Pass through the pre-attention projection: b x lq x (n*dv)\n","        # Separate different heads: b x lq x n x dv\n","        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n","        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n","        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n","\n","        # Transpose for attention dot product: b x n x lq x dv\n","        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n","\n","        if mask is not None:\n","            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n","\n","        q, attn = self.attention(q, k, v, mask=mask)\n","\n","        # Transpose to move the head dimension back: b x lq x n x dv\n","        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n","        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n","        q = self.dropout(self.fc(q))\n","        q += residual\n","\n","        q = self.layer_norm(q)\n","\n","        return q, attn\n","\n","\n","class PositionwiseFeedForward(nn.Module):\n","    ''' A two-feed-forward-layer module '''\n","\n","    def __init__(self, d_in, d_hid, dropout=0.1):\n","        super().__init__()\n","        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n","        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n","        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","\n","        residual = x\n","\n","        x = self.w_2(F.relu(self.w_1(x)))\n","        x = self.dropout(x)\n","        x += residual\n","\n","        x = self.layer_norm(x)\n","\n","        return x\n","\n","\n","def get_subsequent_mask(seq):\n","    ''' For masking out the subsequent info. '''\n","    sz_b, len_s = seq.size()\n","    subsequent_mask = (1 - torch.triu(\n","        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n","    return subsequent_mask\n","\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_hid, n_position=200):\n","        super(PositionalEncoding, self).__init__()\n","\n","        # Not a parameter\n","        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n","\n","    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n","        ''' Sinusoid position encoding table '''\n","        # TODO: make it with torch instead of numpy\n","\n","        def get_position_angle_vec(position):\n","            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n","\n","        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n","        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n","        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n","\n","        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n","\n","    def forward(self, x):\n","        return x + self.pos_table[:, :x.size(1)].clone().detach()\n","\n","\n","class EncoderLayer(nn.Module):\n","    ''' Compose with two layers '''\n","\n","    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.0):\n","        super(EncoderLayer, self).__init__()\n","        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n","        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n","\n","    def forward(self, enc_input, slf_attn_mask=None):\n","        enc_output, enc_slf_attn = self.slf_attn(\n","            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n","        enc_output = self.pos_ffn(enc_output)\n","        return enc_output, enc_slf_attn\n","\n","\n","class TransformerEncoder(nn.Module):\n","    ''' A encoder model with self attention mechanism. '''\n","\n","    def __init__(\n","            self, d_word_vec=512, n_layers=2, n_head=8, d_k=64, d_v=64,\n","            d_model=512, d_inner=2048, dropout=0.1, n_position=624, scale_emb=False):\n","\n","        super().__init__()\n","\n","        # self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx)\n","        if n_position > 0:\n","            self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n","        else:\n","            self.position_enc = lambda x: x\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.layer_stack = nn.ModuleList([\n","            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n","            for _ in range(n_layers)])\n","        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n","        self.scale_emb = scale_emb\n","        self.d_model = d_model\n","\n","    def forward(self, src_seq, src_mask, return_attns=False):\n","\n","        enc_slf_attn_list = []\n","\n","        # -- Forward\n","        # enc_output = self.src_word_emb(src_seq)\n","        enc_output = src_seq\n","        if self.scale_emb:\n","            enc_output *= self.d_model ** 0.5\n","        enc_output = self.dropout(self.position_enc(enc_output))\n","        enc_output = self.layer_norm(enc_output)\n","\n","        for enc_layer in self.layer_stack:\n","            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)\n","            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n","\n","        if return_attns:\n","            return enc_output, enc_slf_attn_list\n","        return enc_output\n","\n","\n","# CleanUNet architecture\n","\n","\n","def padding(x, D, K, S):\n","    \"\"\"padding zeroes to x so that denoised audio has the same length\"\"\"\n","\n","    L = x.shape[-1]\n","    for _ in range(D):\n","        if L < K:\n","            L = 1\n","        else:\n","            L = 1 + np.ceil((L - K) / S)\n","\n","    for _ in range(D):\n","        L = (L - 1) * S + K\n","\n","    L = int(L)\n","    x = F.pad(x, (0, L - x.shape[-1]))\n","    return x\n","\n","\n","class CleanUNet(nn.Module):\n","    \"\"\" CleanUNet architecture. \"\"\"\n","\n","    def __init__(self, channels_input=1, channels_output=1,\n","                 channels_H=64, max_H=768,\n","                 encoder_n_layers=8, kernel_size=4, stride=2,\n","                 tsfm_n_layers=3,\n","                 tsfm_n_head=8,\n","                 tsfm_d_model=512,\n","                 tsfm_d_inner=2048):\n","\n","        \"\"\"\n","        Parameters:\n","        channels_input (int):   input channels\n","        channels_output (int):  output channels\n","        channels_H (int):       middle channels H that controls capacity\n","        max_H (int):            maximum H\n","        encoder_n_layers (int): number of encoder/decoder layers D\n","        kernel_size (int):      kernel size K\n","        stride (int):           stride S\n","        tsfm_n_layers (int):    number of self attention blocks N\n","        tsfm_n_head (int):      number of heads in each self attention block\n","        tsfm_d_model (int):     d_model of self attention\n","        tsfm_d_inner (int):     d_inner of self attention\n","        \"\"\"\n","\n","        super(CleanUNet, self).__init__()\n","\n","        self.channels_input = channels_input\n","        self.channels_output = channels_output\n","        self.channels_H = channels_H\n","        self.max_H = max_H\n","        self.encoder_n_layers = encoder_n_layers\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","\n","        self.tsfm_n_layers = tsfm_n_layers\n","        self.tsfm_n_head = tsfm_n_head\n","        self.tsfm_d_model = tsfm_d_model\n","        self.tsfm_d_inner = tsfm_d_inner\n","\n","        # encoder and decoder\n","        self.encoder = nn.ModuleList()\n","        self.decoder = nn.ModuleList()\n","\n","        for i in range(encoder_n_layers):\n","            self.encoder.append(nn.Sequential(\n","                nn.Conv1d(channels_input, channels_H, kernel_size, stride),\n","                nn.ReLU(),\n","                nn.Conv1d(channels_H, channels_H * 2, 1),\n","                nn.GLU(dim=1)\n","            ))\n","            channels_input = channels_H\n","\n","            if i == 0:\n","                # no relu at end\n","                self.decoder.append(nn.Sequential(\n","                    nn.Conv1d(channels_H, channels_H * 2, 1),\n","                    nn.GLU(dim=1),\n","                    nn.ConvTranspose1d(channels_H, channels_output, kernel_size, stride)\n","                ))\n","            else:\n","                self.decoder.insert(0, nn.Sequential(\n","                    nn.Conv1d(channels_H, channels_H * 2, 1),\n","                    nn.GLU(dim=1),\n","                    nn.ConvTranspose1d(channels_H, channels_output, kernel_size, stride),\n","                    nn.ReLU()\n","                ))\n","            channels_output = channels_H\n","\n","            # double H but keep below max_H\n","            channels_H *= 2\n","            channels_H = min(channels_H, max_H)\n","\n","        # self attention block\n","        self.tsfm_conv1 = nn.Conv1d(channels_output, tsfm_d_model, kernel_size=1)\n","        self.tsfm_encoder = TransformerEncoder(d_word_vec=tsfm_d_model,\n","                                               n_layers=tsfm_n_layers,\n","                                               n_head=tsfm_n_head,\n","                                               d_k=tsfm_d_model // tsfm_n_head,\n","                                               d_v=tsfm_d_model // tsfm_n_head,\n","                                               d_model=tsfm_d_model,\n","                                               d_inner=tsfm_d_inner,\n","                                               dropout=0.0,\n","                                               n_position=0,\n","                                               scale_emb=False)\n","        self.tsfm_conv2 = nn.Conv1d(tsfm_d_model, channels_output, kernel_size=1)\n","\n","        # weight scaling initialization\n","        for layer in self.modules():\n","            if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d)):\n","                weight_scaling_init(layer)\n","\n","    def forward(self, noisy_audio):\n","        # (B, L) -> (B, C, L)\n","        if len(noisy_audio.shape) == 2:\n","            noisy_audio = noisy_audio.unsqueeze(1)\n","        B, C, L = noisy_audio.shape\n","        assert C == 1\n","\n","        # normalization and padding\n","        std = noisy_audio.std(dim=2, keepdim=True) + 1e-3\n","        noisy_audio /= std\n","        x = padding(noisy_audio, self.encoder_n_layers, self.kernel_size, self.stride)\n","\n","        # encoder\n","        skip_connections = []\n","        for downsampling_block in self.encoder:\n","            x = downsampling_block(x)\n","            skip_connections.append(x)\n","        skip_connections = skip_connections[::-1]\n","\n","        # attention mask for causal inference; for non-causal, set attn_mask to None\n","        len_s = x.shape[-1]  # length at bottleneck\n","        attn_mask = (1 - torch.triu(torch.ones((1, len_s, len_s), device=x.device), diagonal=1)).bool()\n","\n","        x = self.tsfm_conv1(x)  # C 1024 -> 512\n","        x = x.permute(0, 2, 1)\n","        x = self.tsfm_encoder(x, src_mask=attn_mask)\n","        x = x.permute(0, 2, 1)\n","        x = self.tsfm_conv2(x)  # C 512 -> 1024\n","\n","        # decoder\n","        for i, upsampling_block in enumerate(self.decoder):\n","            skip_i = skip_connections[i]\n","            x += skip_i[:, :, :x.shape[-1]]\n","            x = upsampling_block(x)\n","\n","        x = x[:, :, :L] * std\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"4G0oSvi3EKtr"},"source":["## Checkpoint Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ihc1YzDrpjEn","outputId":"7cb99fb1-74e1-43aa-89c1-502ffac59a70"},"outputs":[],"source":["!wget https://media.githubusercontent.com/media/NVIDIA/CleanUNet/main/exp/DNS-large-full/checkpoint/pretrained.pkl?download=true"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lisG7uunpqL1"},"outputs":[],"source":["import torch\n","enhance = torch.load(\"/content/pretrained.pkl?download=true\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"snIcJRQcqEbX","outputId":"d1577776-3b67-4972-8791-a7c1f428ef93"},"outputs":[],"source":["enhance.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDklfJpcqJ4m"},"outputs":[],"source":["params = {\n","        \"channels_input\": 1,\n","        \"channels_output\": 1,\n","        \"channels_H\": 64,\n","        \"max_H\": 768,\n","        \"encoder_n_layers\": 8,\n","        \"kernel_size\": 4,\n","        \"stride\": 2,\n","        \"tsfm_n_layers\": 5,\n","        \"tsfm_n_head\": 8,\n","        \"tsfm_d_model\": 512,\n","        \"tsfm_d_inner\": 2048\n","}\n","model_enhance = CleanUNet(**params).cuda()\n","\n","model_enhance.load_state_dict(enhance['model_state_dict'])\n","model_enhance.eval()"]},{"cell_type":"markdown","metadata":{"id":"JGw8oLNFEC2g"},"source":["# ASR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sz_T8YHnRg2z"},"outputs":[],"source":["\"\"\"\n","You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n","\n","Instructions for setting up Colab are as follows:\n","1. Open a new Python 3 notebook.\n","2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n","3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n","4. Run this cell to set up dependencies.\n","5. Restart the runtime (Runtime -> Restart Runtime) for any upgraded packages to take effect\n","\n","\n","NOTE: User is responsible for checking the content of datasets and the applicable licenses and determining if suitable for the intended use.\n","\"\"\"\n","# If you're using Google Colab and not running locally, run this cell.\n","\n","## Install dependencies\n","# !pip install wget\n","!apt-get install -y sox libsndfile1 ffmpeg\n","!pip install text-unidecode\n","# !pip install matplotlib>=3.3.2\n","\n","## Install NeMo\n","BRANCH = 'r2.0.0rc0'\n","!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n","# !pip install nemo_toolkit[all] --upgrade\n","\n","\"\"\"\n","Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n","Alternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\n","that you want to use the \"Run All Cells\" (or similar) option.\n","\"\"\"\n","# exit()"]},{"cell_type":"markdown","metadata":{"id":"3LiOvQHFRk7B"},"source":["## Download Checkpoint from kaggle\n","\n","Link might be expired, please refer to the README file for the latest link"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1IYRlcLRneA","outputId":"6e3b3c1f-2c3d-42f3-ddfa-5d6ed71c477a"},"outputs":[],"source":["!wget https://www.kaggleusercontent.com/kf/185200440/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..9j8hO9RcMQo_hMrxtQ7UPA.jILygHNPhDmiiFkzrJcsUXvt84JgTLyHfRx6lqFPjKZkImx4pQ9BFefXPewyzRArFDpelTPV57kj1wWvgxbSalqbMq_E2xPefntPwFHqcvpknzG7E4iuZOoHvrQlNSaulSJAZgmIj3K2oj4qlOXMxO8J276k0EzWNdXmMMiWkTdpdn2GnxJ0K_J7r32Ishj1cGWVASzijnB-oas6h2Zw7yKbAJMdwM0Kcfrs-__VjgYku3J7vwjEki-Jjn0ClqyegssAx2yAU26M3XEZDN9B6sT1GeB422rkynfeBHwnlw3V7VPaJZ6Ywe2x-Hv7dLMu48mjfzCourrjC70Xu5dBPHzTveTwhRu1sGfmo_oMCEgsOoWVnrQLIuAwTjs8LnJpvmtOB2p_SsnG-j7byRTs3monO5mQP3EAEHAAyzUwdUD4quEAu2FG9qteWOF67YjOOEeNBA9OQwGrkQvKAgvCHza6kP0BYUhF4yvXwrlnHv90_fOW7lftUh7ktZzrDURQvudZVtxWn5OCjG5OJ9wOTjrnDY0ThKVS-qp6X3E7shYluqtylrYpF9_cj9CmvwdWDmUHBR2U8RObQKQyvaFAAyQH22SecCs4NHHXLievwR0AKnQ4xff6Fr10X44lHVE-ETrN_oglWf_U288v-5xzfQ.MiYrc-NVZdn_bTKteMYVxQ/AIC-ASR/i9bfrpip/checkpoints/epoch=215-step=21600.ckpt"]},{"cell_type":"markdown","metadata":{"id":"Tto0pAnkStgb"},"source":["## Tokenizer Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVEfOs3OR5ub"},"outputs":[],"source":["BRANCH = 'r2.0.0rc0'\n","if not os.path.exists(\"scripts/tokenizers/process_asr_text_tokenizer.py\"):\n","  !mkdir scripts\n","  !wget -P scripts/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tokenizers/process_asr_text_tokenizer.py\n","\n","!python ./scripts/process_asr_text_tokenizer.py \\\n","  --manifest=\"/content/train_manifest.json\" \\\n","  --data_root=\".\" \\\n","  --vocab_size=256 \\\n","  --tokenizer=\"spe\" \\\n","  --no_lower_case \\\n","  --spe_type=\"bpe\" \\\n","  --log"]},{"cell_type":"markdown","metadata":{"id":"M1whDCx9SxkE"},"source":["## Model Configs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykzInJEVR7lg","outputId":"3be87463-e33b-4eca-dd58-77e0903ad96f"},"outputs":[],"source":["try:\n","    from ruamel.yaml import YAML\n","except ModuleNotFoundError:\n","    from ruamel_yaml import YAML\n","config_path = './configs/fast-conformer_ctc_bpe.yaml'\n","\n","if not os.path.exists(config_path):\n","    # Grab the config we'll use in this example\n","    BRANCH = 'r2.0.0rc0'\n","    !mkdir configs\n","    !wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/fastconformer/fast-conformer_ctc_bpe.yaml\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TThpuKBfR9N5","outputId":"56903813-2fe2-4ea0-8c7d-369492247dbf"},"outputs":[],"source":["from omegaconf import OmegaConf\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","import logging\n","\n","# NeMo's \"core\" package\n","import nemo\n","# NeMo's ASR collection - this collections contains complete ASR models and\n","# building blocks (modules) for ASR\n","import nemo.collections.asr as nemo_asr\n","\n","\n","# --- Config Information ---#\n","try:\n","    from ruamel.yaml import YAML\n","except ModuleNotFoundError:\n","    from ruamel_yaml import YAML\n","config_path = './configs/fast-conformer_ctc_bpe.yaml'\n","\n","yaml = YAML(typ='safe')\n","with open(config_path) as f:\n","    params = yaml.load(f)\n","print(params)\n","\n","train_manifest = \"/content/train_manifest.json\"\n","adapt_manifest = \"/content/train_manifest.json\"\n","\n","\n","params['model']['train_ds']['manifest_filepath'] = train_manifest\n","params['model']['train_ds']['batch_size'] = 16\n","params['model']['train_ds']['num_workers'] = 4\n","\n","\n","params['model']['validation_ds']['manifest_filepath'] = None\n","params['model']['validation_ds']['batch_size'] = 1\n","params['model']['validation_ds']['num_workers'] = 4\n","\n","\n","params['model']['tokenizer']['dir'] = \"/content/tokenizer_spe_bpe_v256\"\n","params['model']['optim']['sched']['warmup_steps'] = 0\n","\n","params['model']['optim']['lr'] = 5e-5\n","params['model']['optim']['weight_decay'] = 0\n","params['model']['optim']['sched']['warmup_steps'] = 0\n","params['model']['optim']['sched']['min_lr'] = 1e-6\n","\n","\n","params['model'].pop('test_ds')"]},{"cell_type":"markdown","metadata":{"id":"lijWorH3S1KK"},"source":["## Model Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSqS78JNSA2Z","outputId":"028ba008-ba80-4bc8-b8f8-9db250c4d475"},"outputs":[],"source":["import nemo.collections.asr as nemo_asr\n","conf = OmegaConf.create(params)\n","model = nemo_asr.models.EncDecCTCModelBPE(cfg=conf['model'])"]},{"cell_type":"markdown","metadata":{"id":"x4NaRkhNSS81"},"source":["### Loading Checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8SNoGf5ASKDL"},"outputs":[],"source":["ckpt = torch.load(\"/content/epoch=215-step=21600.ckpt\")\n","model.load_state_dict(ckpt['state_dict'])"]},{"cell_type":"markdown","metadata":{"id":"496kTG1jSjCS"},"source":["## Testing on train/adapt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"id":"VAiFpuOKScRs","outputId":"f876ce26-190d-4137-a577-1b31c7a531a1"},"outputs":[],"source":["import soundfile as sf\n","from IPython.display import Audio\n","audio_path = \"test/test_sample_3577_noisy.wav\"\n","model.eval()\n","# model = model.cuda()\n","with torch.no_grad():\n","  clean = model_enhance(torch.Tensor(sf.read(audio_path, dtype='float32')[0]).cuda().unsqueeze(0).unsqueeze(0)).squeeze()\n","  print(clean.shape)\n","  print(model.transcribe([clean]))\n","  print(model.transcribe([sf.read(audio_path, dtype='float32')[0]]))\n","# print(sf.read(audio_path, dtype='float32'))\n","Audio(clean.cpu().numpy(), rate=16000)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"HCLfa_xN2MbA","outputId":"500086ef-a503-43ae-85d8-d507c7f44e66"},"outputs":[],"source":["Audio(audio_path)"]},{"cell_type":"markdown","metadata":{"id":"UtIUOtTfSenx"},"source":["## Test results.csv generation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["56599d95b70a4b02be9e093627c6d60d","4b0ad3e81a7e42b492773801569545c7","ef2a95e7184c47559fceb1beb32cea63","d153d16e52794277887599b6ef2ff64d","a14773a3c37b4515af2ba4f6bba1ebbc","a5967d0b00b140f387b17ab5a2587500","073528ece580470cb7a577a248b2ddb3","e4548ada913448688725dd4ea96f9a87","ad96f3300f7249a29c96f1f836afd15f","3b8d56e4d04f4cbba7809cbd0e02c7b5","198496311cbb497da9d510e8af2f7860"]},"collapsed":true,"id":"6_gWRU-ZSd67","jupyter":{"outputs_hidden":true},"outputId":"193d1865-ccab-4eca-9208-00b0b6dab229"},"outputs":[],"source":["import torch\n","import soundfile as sf\n","import os\n","\n","data_dir = \"/content/test\"\n","\n","model.eval()\n","\n","with open(\"results.csv\", \"w+\", encoding='utf-8') as fp:\n","    fp.write(\"audio,transcript\\n\")\n","\n","count = 0\n","\n","for filename in tqdm(os.listdir(data_dir)):\n","    audio, sr = sf.read(os.path.join(data_dir, filename), dtype='float32')\n","    with torch.no_grad():\n","        clean = model_enhance(torch.Tensor(audio).unsqueeze(0).unsqueeze(0).cuda()).squeeze()\n","        rv = model.transcribe([clean])\n","    with open(\"results.csv\", \"a+\") as fp:\n","        fp.write(f\"{os.path.splitext(os.path.basename(filename))[0]},{rv[0]}\\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["AnJODp_pEGgo","4G0oSvi3EKtr","3LiOvQHFRk7B","OOfIxv1NRxm5","Tto0pAnkStgb","M1whDCx9SxkE","lijWorH3S1KK"],"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5289407,"sourceId":8796530,"sourceType":"datasetVersion"},{"datasetId":5295858,"sourceId":8805534,"sourceType":"datasetVersion"},{"datasetId":5136201,"sourceId":8811978,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"073528ece580470cb7a577a248b2ddb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"198496311cbb497da9d510e8af2f7860":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b8d56e4d04f4cbba7809cbd0e02c7b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b0ad3e81a7e42b492773801569545c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5967d0b00b140f387b17ab5a2587500","placeholder":"​","style":"IPY_MODEL_073528ece580470cb7a577a248b2ddb3","value":"100%"}},"56599d95b70a4b02be9e093627c6d60d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b0ad3e81a7e42b492773801569545c7","IPY_MODEL_ef2a95e7184c47559fceb1beb32cea63","IPY_MODEL_d153d16e52794277887599b6ef2ff64d"],"layout":"IPY_MODEL_a14773a3c37b4515af2ba4f6bba1ebbc"}},"a14773a3c37b4515af2ba4f6bba1ebbc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5967d0b00b140f387b17ab5a2587500":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad96f3300f7249a29c96f1f836afd15f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d153d16e52794277887599b6ef2ff64d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b8d56e4d04f4cbba7809cbd0e02c7b5","placeholder":"​","style":"IPY_MODEL_198496311cbb497da9d510e8af2f7860","value":" 1726/1726 [03:49&lt;00:00,  6.84it/s]"}},"e4548ada913448688725dd4ea96f9a87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef2a95e7184c47559fceb1beb32cea63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4548ada913448688725dd4ea96f9a87","max":1726,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad96f3300f7249a29c96f1f836afd15f","value":1726}}}}},"nbformat":4,"nbformat_minor":4}
